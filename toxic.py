# -*- coding: utf-8 -*-
"""toxic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vYUtD2Ye_TFXnQWgIY7pARxkaxdJdjp
"""




import streamlit as st
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the trained model
model = tf.keras.models.load_model("toxic_comment_lstm_model.h5")

# Constants (use the same values you used in training)
MAX_NUM_WORDS = 20000
MAX_SEQUENCE_LENGTH = 150

# Load the tokenizer (you can save and load it similar to the model if needed)
# For simplicity, let's create a new tokenizer instance
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)

def predict_toxicity(comment):
    # Preprocess the comment
    sequences = tokenizer.texts_to_sequences([comment])  # Use 'comment' instead of 'comment_text'
    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    prediction = model.predict(data)
    return prediction[0][0]

# Streamlit app layout
st.title("Toxic Comment Classification")
comment_input = st.text_area("Enter your comment here:")

if st.button("Predict"):
    if comment_input:
        prediction = predict_toxicity(comment_input)
        result = "Toxic" if prediction >= 0.5 else "Non-Toxic"
        st.write(f"Prediction: {result} (Confidence: {prediction:.2f})")
    else:
        st.write("Please enter a comment.")
